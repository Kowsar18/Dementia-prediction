{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSdOseT2UrvC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HJahPk2oyU4",
        "outputId": "8e72160d-7db0-4797-e530-586097fe17d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6397 - loss: 0.6942 - val_accuracy: 0.7776 - val_loss: 0.5990\n",
            "Epoch 2/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7459 - loss: 0.5500 - val_accuracy: 0.7613 - val_loss: 0.5642\n",
            "Epoch 3/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7684 - loss: 0.4985 - val_accuracy: 0.7866 - val_loss: 0.5287\n",
            "Epoch 4/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7374 - loss: 0.5157 - val_accuracy: 0.7830 - val_loss: 0.4852\n",
            "Epoch 5/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7521 - loss: 0.4921 - val_accuracy: 0.8083 - val_loss: 0.4489\n",
            "Epoch 6/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7682 - loss: 0.4837 - val_accuracy: 0.8119 - val_loss: 0.4190\n",
            "Epoch 7/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7657 - loss: 0.4837 - val_accuracy: 0.8210 - val_loss: 0.4007\n",
            "Epoch 8/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7882 - loss: 0.4627 - val_accuracy: 0.8228 - val_loss: 0.3868\n",
            "Epoch 9/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.4616 - val_accuracy: 0.8373 - val_loss: 0.3756\n",
            "Epoch 10/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7853 - loss: 0.4551 - val_accuracy: 0.8391 - val_loss: 0.3651\n",
            "Epoch 11/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7699 - loss: 0.4638 - val_accuracy: 0.8535 - val_loss: 0.3537\n",
            "Epoch 12/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7916 - loss: 0.4412 - val_accuracy: 0.8590 - val_loss: 0.3487\n",
            "Epoch 13/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7896 - loss: 0.4241 - val_accuracy: 0.8608 - val_loss: 0.3496\n",
            "Epoch 14/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8027 - loss: 0.4276 - val_accuracy: 0.8517 - val_loss: 0.3392\n",
            "Epoch 15/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7974 - loss: 0.4174 - val_accuracy: 0.8590 - val_loss: 0.3333\n",
            "Epoch 16/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8174 - loss: 0.3948 - val_accuracy: 0.8698 - val_loss: 0.3256\n",
            "Epoch 17/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8011 - loss: 0.4059 - val_accuracy: 0.8698 - val_loss: 0.3192\n",
            "Epoch 18/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8261 - loss: 0.3933 - val_accuracy: 0.8861 - val_loss: 0.3068\n",
            "Epoch 19/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8071 - loss: 0.3929 - val_accuracy: 0.8897 - val_loss: 0.3030\n",
            "Epoch 20/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8242 - loss: 0.3703 - val_accuracy: 0.8861 - val_loss: 0.3000\n",
            "Epoch 21/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8275 - loss: 0.3741 - val_accuracy: 0.8879 - val_loss: 0.2912\n",
            "Epoch 22/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8268 - loss: 0.3635 - val_accuracy: 0.8969 - val_loss: 0.2855\n",
            "Epoch 23/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8515 - loss: 0.3376 - val_accuracy: 0.8879 - val_loss: 0.2870\n",
            "Epoch 24/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8277 - loss: 0.3682 - val_accuracy: 0.8933 - val_loss: 0.2814\n",
            "Epoch 25/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8311 - loss: 0.3864 - val_accuracy: 0.8843 - val_loss: 0.2794\n",
            "Epoch 26/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8379 - loss: 0.3620 - val_accuracy: 0.8933 - val_loss: 0.2711\n",
            "Epoch 27/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8324 - loss: 0.3612 - val_accuracy: 0.8825 - val_loss: 0.2701\n",
            "Epoch 28/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8570 - loss: 0.3420 - val_accuracy: 0.8969 - val_loss: 0.2682\n",
            "Epoch 29/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8438 - loss: 0.3465 - val_accuracy: 0.9096 - val_loss: 0.2662\n",
            "Epoch 30/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.3554 - val_accuracy: 0.9005 - val_loss: 0.2597\n",
            "Epoch 31/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8505 - loss: 0.3212 - val_accuracy: 0.8879 - val_loss: 0.2565\n",
            "Epoch 32/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8425 - loss: 0.3434 - val_accuracy: 0.8987 - val_loss: 0.2612\n",
            "Epoch 33/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8669 - loss: 0.3055 - val_accuracy: 0.8897 - val_loss: 0.2557\n",
            "Epoch 34/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8572 - loss: 0.3404 - val_accuracy: 0.9005 - val_loss: 0.2465\n",
            "Epoch 35/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8680 - loss: 0.3272 - val_accuracy: 0.8969 - val_loss: 0.2524\n",
            "Epoch 36/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8585 - loss: 0.3280 - val_accuracy: 0.8987 - val_loss: 0.2503\n",
            "Epoch 37/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8581 - loss: 0.3235 - val_accuracy: 0.8897 - val_loss: 0.2487\n",
            "Epoch 38/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8737 - loss: 0.2931 - val_accuracy: 0.9005 - val_loss: 0.2454\n",
            "Epoch 39/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8722 - loss: 0.3101 - val_accuracy: 0.9060 - val_loss: 0.2405\n",
            "Epoch 40/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8462 - loss: 0.3045 - val_accuracy: 0.9078 - val_loss: 0.2334\n",
            "Epoch 41/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8589 - loss: 0.3056 - val_accuracy: 0.9078 - val_loss: 0.2298\n",
            "Epoch 42/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8291 - loss: 0.3388 - val_accuracy: 0.9096 - val_loss: 0.2347\n",
            "Epoch 43/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8670 - loss: 0.3042 - val_accuracy: 0.8951 - val_loss: 0.2295\n",
            "Epoch 44/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8606 - loss: 0.3080 - val_accuracy: 0.9132 - val_loss: 0.2325\n",
            "Epoch 45/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8670 - loss: 0.2884 - val_accuracy: 0.9024 - val_loss: 0.2273\n",
            "Epoch 46/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8641 - loss: 0.3007 - val_accuracy: 0.9114 - val_loss: 0.2334\n",
            "Epoch 47/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8633 - loss: 0.3068 - val_accuracy: 0.9096 - val_loss: 0.2294\n",
            "Epoch 48/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8983 - loss: 0.2586 - val_accuracy: 0.9078 - val_loss: 0.2218\n",
            "Epoch 49/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8605 - loss: 0.2925 - val_accuracy: 0.9132 - val_loss: 0.2220\n",
            "Epoch 50/50\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8655 - loss: 0.2944 - val_accuracy: 0.9042 - val_loss: 0.2312\n",
            "MLP Test Accuracy: 89.87%\n",
            "MLP Classification Report:\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.83      0.90       370\n",
            "         1.0       0.83      0.98      0.90       321\n",
            "\n",
            "    accuracy                           0.90       691\n",
            "   macro avg       0.90      0.90      0.90       691\n",
            "weighted avg       0.91      0.90      0.90       691\n",
            "\n",
            "Epoch 1/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6054 - loss: 0.8737 - val_accuracy: 0.7740 - val_loss: 0.6459\n",
            "Epoch 2/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7277 - loss: 0.5847 - val_accuracy: 0.8065 - val_loss: 0.5756\n",
            "Epoch 3/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7322 - loss: 0.5460 - val_accuracy: 0.8119 - val_loss: 0.5001\n",
            "Epoch 4/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7657 - loss: 0.4999 - val_accuracy: 0.8300 - val_loss: 0.4272\n",
            "Epoch 5/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7648 - loss: 0.4902 - val_accuracy: 0.8427 - val_loss: 0.3782\n",
            "Epoch 6/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7725 - loss: 0.4855 - val_accuracy: 0.8517 - val_loss: 0.3440\n",
            "Epoch 7/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7986 - loss: 0.4272 - val_accuracy: 0.8590 - val_loss: 0.3281\n",
            "Epoch 8/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7983 - loss: 0.4231 - val_accuracy: 0.8644 - val_loss: 0.3143\n",
            "Epoch 9/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8313 - loss: 0.3840 - val_accuracy: 0.8590 - val_loss: 0.2996\n",
            "Epoch 10/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8158 - loss: 0.3891 - val_accuracy: 0.8644 - val_loss: 0.2927\n",
            "Epoch 11/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8260 - loss: 0.3519 - val_accuracy: 0.8626 - val_loss: 0.3016\n",
            "Epoch 12/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8290 - loss: 0.3741 - val_accuracy: 0.8788 - val_loss: 0.2753\n",
            "Epoch 13/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8329 - loss: 0.3746 - val_accuracy: 0.8752 - val_loss: 0.2717\n",
            "Epoch 14/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8319 - loss: 0.3604 - val_accuracy: 0.8788 - val_loss: 0.2619\n",
            "Epoch 15/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8367 - loss: 0.3584 - val_accuracy: 0.8843 - val_loss: 0.2646\n",
            "Epoch 16/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8330 - loss: 0.3544 - val_accuracy: 0.8879 - val_loss: 0.2637\n",
            "Epoch 17/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8466 - loss: 0.3051 - val_accuracy: 0.8897 - val_loss: 0.2533\n",
            "Epoch 18/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8505 - loss: 0.3228 - val_accuracy: 0.8861 - val_loss: 0.2569\n",
            "Epoch 19/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8337 - loss: 0.3519 - val_accuracy: 0.8933 - val_loss: 0.2472\n",
            "Epoch 20/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8660 - loss: 0.3112 - val_accuracy: 0.8915 - val_loss: 0.2425\n",
            "Epoch 21/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8447 - loss: 0.3308 - val_accuracy: 0.8933 - val_loss: 0.2425\n",
            "Epoch 22/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8598 - loss: 0.3015 - val_accuracy: 0.8969 - val_loss: 0.2335\n",
            "Epoch 23/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8633 - loss: 0.3123 - val_accuracy: 0.8933 - val_loss: 0.2377\n",
            "Epoch 24/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8671 - loss: 0.2933 - val_accuracy: 0.8933 - val_loss: 0.2274\n",
            "Epoch 25/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8464 - loss: 0.3368 - val_accuracy: 0.8987 - val_loss: 0.2303\n",
            "Epoch 26/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8771 - loss: 0.2852 - val_accuracy: 0.8951 - val_loss: 0.2250\n",
            "Epoch 27/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8935 - loss: 0.2689 - val_accuracy: 0.8969 - val_loss: 0.2225\n",
            "Epoch 28/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8574 - loss: 0.3021 - val_accuracy: 0.8969 - val_loss: 0.2239\n",
            "Epoch 29/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8708 - loss: 0.2911 - val_accuracy: 0.9024 - val_loss: 0.2258\n",
            "Epoch 30/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8766 - loss: 0.2843 - val_accuracy: 0.9042 - val_loss: 0.2173\n",
            "Epoch 31/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8703 - loss: 0.2763 - val_accuracy: 0.8987 - val_loss: 0.2191\n",
            "Epoch 32/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8877 - loss: 0.2544 - val_accuracy: 0.9042 - val_loss: 0.2116\n",
            "Epoch 33/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8819 - loss: 0.2723 - val_accuracy: 0.9060 - val_loss: 0.2127\n",
            "Epoch 34/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8886 - loss: 0.2522 - val_accuracy: 0.9060 - val_loss: 0.2157\n",
            "Epoch 35/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8831 - loss: 0.2716 - val_accuracy: 0.9078 - val_loss: 0.2067\n",
            "Epoch 36/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8939 - loss: 0.2511 - val_accuracy: 0.9005 - val_loss: 0.2108\n",
            "Epoch 37/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8920 - loss: 0.2419 - val_accuracy: 0.8933 - val_loss: 0.2170\n",
            "Epoch 38/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8887 - loss: 0.2484 - val_accuracy: 0.9042 - val_loss: 0.2138\n",
            "Epoch 39/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8887 - loss: 0.2505 - val_accuracy: 0.9114 - val_loss: 0.2034\n",
            "Epoch 40/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8760 - loss: 0.2801 - val_accuracy: 0.8969 - val_loss: 0.2115\n",
            "Epoch 41/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8915 - loss: 0.2521 - val_accuracy: 0.9078 - val_loss: 0.2115\n",
            "Epoch 42/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9122 - loss: 0.2199 - val_accuracy: 0.9060 - val_loss: 0.2072\n",
            "Epoch 43/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9021 - loss: 0.2529 - val_accuracy: 0.9150 - val_loss: 0.2103\n",
            "Epoch 44/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8975 - loss: 0.2458 - val_accuracy: 0.9096 - val_loss: 0.2034\n",
            "Epoch 45/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8850 - loss: 0.2402 - val_accuracy: 0.9005 - val_loss: 0.2015\n",
            "Epoch 46/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8928 - loss: 0.2491 - val_accuracy: 0.9114 - val_loss: 0.2001\n",
            "Epoch 47/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9022 - loss: 0.2229 - val_accuracy: 0.9078 - val_loss: 0.2004\n",
            "Epoch 48/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9007 - loss: 0.2271 - val_accuracy: 0.9150 - val_loss: 0.1917\n",
            "Epoch 49/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9068 - loss: 0.2143 - val_accuracy: 0.9186 - val_loss: 0.1913\n",
            "Epoch 50/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9061 - loss: 0.2225 - val_accuracy: 0.9114 - val_loss: 0.2011\n",
            "Epoch 51/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8976 - loss: 0.2355 - val_accuracy: 0.9078 - val_loss: 0.1974\n",
            "Epoch 52/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9276 - loss: 0.1922 - val_accuracy: 0.9114 - val_loss: 0.2007\n",
            "Epoch 53/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9014 - loss: 0.2199 - val_accuracy: 0.9222 - val_loss: 0.1847\n",
            "Epoch 54/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9038 - loss: 0.2165 - val_accuracy: 0.9114 - val_loss: 0.1945\n",
            "Epoch 55/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9136 - loss: 0.2141 - val_accuracy: 0.9186 - val_loss: 0.1822\n",
            "Epoch 56/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8972 - loss: 0.2392 - val_accuracy: 0.9168 - val_loss: 0.1911\n",
            "Epoch 57/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9205 - loss: 0.2151 - val_accuracy: 0.9114 - val_loss: 0.1918\n",
            "Epoch 58/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9111 - loss: 0.2078 - val_accuracy: 0.9096 - val_loss: 0.1865\n",
            "Epoch 59/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8993 - loss: 0.2387 - val_accuracy: 0.9132 - val_loss: 0.1905\n",
            "Epoch 60/60\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9198 - loss: 0.1944 - val_accuracy: 0.9132 - val_loss: 0.1904\n",
            "Hybrid Neural Network (HNN) Test Accuracy: 91.17%\n",
            "HNN Classification Report:\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.86      0.91       370\n",
            "         1.0       0.86      0.97      0.91       321\n",
            "\n",
            "    accuracy                           0.91       691\n",
            "   macro avg       0.91      0.92      0.91       691\n",
            "weighted avg       0.92      0.91      0.91       691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Input, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Multilayer Perceptron (MLP) Model\n",
        "\n",
        "model_mlp = Sequential([\n",
        "    Dense(256, input_dim=X_train.shape[1], activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_mlp.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "history_mlp = model_mlp.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "\n",
        "mlp_loss, mlp_accuracy = model_mlp.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"MLP Test Accuracy: {mlp_accuracy * 100:.2f}%\")\n",
        "print(\"MLP Classification Report:\")\n",
        "print(classification_report(y_test, (model_mlp.predict(X_test) > 0.5).astype(int)))\n",
        "\n",
        "\n",
        "# Hybrid Neural Network (HNN) Model: CNN + MLP Layers\n",
        "\n",
        "X_train_cnn = np.expand_dims(X_train, axis=2)\n",
        "X_test_cnn = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "input_layer = Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
        "conv_layer = Conv1D(64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
        "conv_layer = BatchNormalization()(conv_layer)\n",
        "conv_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "conv_layer = Dropout(0.3)(conv_layer)\n",
        "conv_layer = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv_layer)\n",
        "conv_layer = BatchNormalization()(conv_layer)\n",
        "conv_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "conv_layer = Dropout(0.3)(conv_layer)\n",
        "flatten = Flatten()(conv_layer)\n",
        "dense_layer = Dense(128, activation='relu')(flatten)\n",
        "dense_layer = Dropout(0.4)(dense_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "\n",
        "model_hnn = Model(inputs=input_layer, outputs=output_layer)\n",
        "model_hnn.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history_hnn = model_hnn.fit(X_train_cnn, y_train, epochs=60, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "\n",
        "hnn_loss, hnn_accuracy = model_hnn.evaluate(X_test_cnn, y_test, verbose=0)\n",
        "print(f\"Hybrid Neural Network (HNN) Test Accuracy: {hnn_accuracy * 100:.2f}%\")\n",
        "print(\"HNN Classification Report:\")\n",
        "print(classification_report(y_test, (model_hnn.predict(X_test_cnn) > 0.5).astype(int)))\n"
      ]
    }
  ]
}